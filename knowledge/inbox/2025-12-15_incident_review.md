# 推薦ログ欠損 障害レビュー

**日時**: 2025-12-15 16:00-17:00（緊急招集）
**場所**: オンライン（Zoom）
**参加者**: 田中（PM）、鈴木（データ）、佐藤（エンジニア）、木村（インフラ）

---

## 目的
12/14に発生した推薦ログ欠損インシデントの原因分析と再発防止策の合意

## アジェンダ
1. 障害の概要とタイムライン
2. 影響範囲
3. 原因分析
4. 復旧対応
5. 再発防止策
6. 次アクション

---

## 議事内容

### 1. 障害の概要とタイムライン

| 時刻 | イベント |
|------|---------|
| 12/14 02:00 | バッチ処理が異常終了 |
| 12/14 09:30 | 鈴木がログ欠損を発見（朝のダッシュボード確認時） |
| 12/14 09:45 | 木村に連絡、調査開始 |
| 12/14 11:00 | 原因特定（ディスク容量不足） |
| 12/14 12:00 | 復旧作業完了 |
| 12/14 14:00 | 欠損データの再取得開始（8時間分） |

**欠損期間**: 12/14 02:00 〜 12:00（約10時間）

### 2. 影響範囲

**直接影響**:
- 推薦ログ約120万件の欠損
- DAUの約8%（35,000ユーザー）の行動データが欠落
- A/Bテスト結果の統計精度に影響

**PJへの影響**:
- 12/14分のA/Bテストデータが使用不可
- 実験期間を2日延長する必要あり → 1/17完了予定が1/19に

**金額換算**:
- 復旧作業の工数: 2人日（約10万円相当）
- 実験延長による機会損失: 算定困難

### 3. 原因分析（5 Whys）

**Why 1**: バッチ処理が異常終了した
**Why 2**: ディスク容量が100%に達した
**Why 3**: 古いログファイルが削除されていなかった
**Why 4**: ログローテーション設定が7日→30日に変更されていた
**Why 5**: 11月のセキュリティ対応時に設定変更、元に戻し忘れ

**根本原因**: 設定変更の管理プロセスが不十分

### 4. 復旧対応

**佐藤・木村の対応**:
1. 不要ログファイルの手動削除（50GB解放）
2. バッチ処理の再実行
3. 欠損期間のログを別ソース（CDN）から補完
4. データ整合性の確認

**結果**:
- 120万件中、約100万件（83%）を復旧
- 残り20万件は復旧不可（CDNに記録なし）

### 5. 再発防止策

**短期（1週間以内）**:
- [ ] ディスク使用率80%でアラート設定（木村、12/17）
- [ ] ログローテーション設定を7日に戻す（木村、12/16）
- [ ] 設定変更履歴のドキュメント化（佐藤、12/18）

**中期（1ヶ月以内）**:
- [ ] 設定変更のレビュープロセス導入（チーム全体）
- [ ] ログ欠損検知の自動アラート（鈴木、1月中）

**長期（3ヶ月以内）**:
- [ ] ログ基盤のクラウド移行検討（インフラチーム）

---

## 決定事項
- ログローテーションを7日に即時復旧
- ディスク監視アラートを80%閾値で設定
- A/Bテスト期間を2日延長（1/19まで）
- 設定変更はPR形式でレビュー必須とする

## 未決事項
- クラウド移行の予算・スケジュール（1月の定例で議論）
- 欠損20万件の影響度評価（鈴木が12/18までに分析）

## リスク
- 類似のインシデントが他のバッチ処理でも発生する可能性
- 設定管理の属人化が解消されていない

## 教訓
- 「一時的な設定変更」は戻し忘れリスクが高い
- 監視アラートがないシステムは障害発見が遅れる
- ログの冗長化（別ソース保存）が有効だった

## 次アクション
| アクション | 担当 | 期限 |
|-----------|------|------|
| ログローテーション設定修正 | 木村 | 12/16 |
| ディスク監視アラート設定 | 木村 | 12/17 |
| 設定変更履歴ドキュメント作成 | 佐藤 | 12/18 |
| 欠損データ影響分析 | 鈴木 | 12/18 |
| インシデントレポート最終版作成 | 田中 | 12/19 |

---

*フォローアップ: 12/22 週次MTGで進捗確認*
